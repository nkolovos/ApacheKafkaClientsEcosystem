\chapter{Conclusions}
\label{chap6}
In this thesis, we developed a full-stack air pollution monitoring system adhering to IoT fundamentals, but also combined with capabilities found in enterprise cloud services. Our key goals and challenges were to extend the capabilities of the SYNAISTHISI platform with advanced event streaming services, create a live environmental air quality monitoring application and make our platform as scalable and interconnected as possible to host other similar applications.

We analyzed our motivations, considering the need to create air pollution monitoring applications and infrastructures accessible to the public. We created an event-driven middle-ware capable of interconnecting with most of the established protocols in the IoT industry. We configured our Kafka cluster in a containerized, multi-broker implementation and adopted the latest control plane KRaft, for managing our Kafka nodes. Furthermore, we interconnected our primary cluster with Kafka Connect and its external communication protocols like AMQP and MQTT, showcasing that almost any other supported external system, service, or database can be connected as well. We added a Schema Registry to our data flow, making our application schema-enabled and providing us with features of advanced data validation, integrity and controlled data evolution. We developed a back-end service that fetches our data from our Kafka Cluster in real-time, eliminating the need for an additional database for our live event messages streaming. We facilitated a Web-Sockets application over a TCP channel, to parse the consumed data to our front-end map, efficiently matching the high throughput requirements of our platform. Last but not least, we created a React-based live map utilizing Leaflet maps, where each device and sensor is automatically added as a point on the map. Each sensor is updated on our live map and a dashboard of tables for each corresponding sensor provides us with historical measurements for up to a week.

By utilizing and orchestrating so many different services, we focused on making them as independent and modular as possible. This way, our application remains highly scalable, accessible and configurable. The whole stack is constructed and presented in a way that showcases each layer, highlighting the re-usability and adoption of our platform in other respective projects. Additionally, each individual service was chosen with the aspect of belonging to an open-source project that is being maintained, further future-proofing our platform. Features such as fault tolerance, data validation, high throughput and data integrity, which are typically found in paid enterprise cloud solutions, were incorporated into our platform. This was achieved by carefully selecting the appropriate open-source projects and services and then optimizing and configuring them to meet our requirements. Therefore, we developed a platform that is accessible to everyone and aligns with the key IoT and open-source principles, while at the same time incorporating advanced cloud features and fundamentals.

\section{Future work}

Regarding future work, there are several improvements and upgrades that could be applied to our implementation. Initially the development of enterprise-level security by utilizing SASL\_SSL and TLS(SSL) security protocols. Our configuration would involve using SSL for internal listeners(clients) and inter-broker communications. Additionally, external listeners(clients) could be configured to utilize SASL\_SSL, integrated with a Kerberos server through the GSSAPI authentication mechanism. Such integration necessitates thorough research and testing based on the cluster's needs and priorities, in order to achieve the optimal balance between security and performance. Additionally, we can enhance the default behavior of Kafka's key mapping mechanism. This could be achieved by integrating an external load balancing service or by developing and configuring our own key hashing mechanism, to distribute the events across the partitions equally and efficiently. 